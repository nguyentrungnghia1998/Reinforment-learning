{"cells":[{"cell_type":"markdown","metadata":{"id":"vH0Drea2Weni"},"source":["# Reinforcement Learning - Deep Q Learning\n","\n","> In this lab, you're going to:\n","\n","1.   Use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v1 task from the [OpenAI Gym](https://gym.openai.com/).\n","2.   Use **experience replay memory** for training your DQN agent.\n","3.   Create a simple plot to visualize your agent's efficiency."]},{"cell_type":"markdown","metadata":{"id":"fB30_MnccddH"},"source":["### Description\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tC3thUnzgegj"},"source":["In Cartpole, a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pole starts upright, and the goal is to prevent it from falling over. The system is controlled by applying a force of +1 or -1 to the cart. A reward of +1 is provided for every timestep that the pole remains upright. The **episode ends** when **the pole is more than 15 degrees from vertical**, or **the cart moves more than 2.4 units from the center of the track**.\n","\n","![](https://pytorch.org/tutorials/_images/cartpole.gif)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":678,"status":"ok","timestamp":1627380445284,"user":{"displayName":"Thanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg44wFhQxb1lNqczFaNjv6DfabszK-OJIJ4ACM6uw=s64","userId":"07489558485061570174"},"user_tz":-420},"id":"-qe6qxFDcwxU","outputId":"5d589356-a446-44e7-ac56-055a4e29b878"},"outputs":[],"source":["### Instantiate the Cartpole environment ###\n","import gym\n","env = gym.make(\"CartPole-v1\")\n","env.seed(1)"]},{"cell_type":"markdown","metadata":{"id":"70N5Yga3fyEt"},"source":["Given this setup for the environment and the objective of the game, we can think about: \n","- 1) What observations help define the environment's state;\n","- 2) What actions the agent can take.\n"]},{"cell_type":"markdown","metadata":{"id":"Dd7gwYncelO4"},"source":["First, let's consider the observation space. In this Cartpole environment, our observations are:\n","- Cart position\n","- Cart velocity\n","- Pole angle\n","- Pole rotation rate\n","\n","We can confirm the size of the space by querying the environment's observation space:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1627380445696,"user":{"displayName":"Thanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg44wFhQxb1lNqczFaNjv6DfabszK-OJIJ4ACM6uw=s64","userId":"07489558485061570174"},"user_tz":-420},"id":"h486iGJtekYT","outputId":"08b7980f-5a61-4ddb-bd87-9db4df8dae3f"},"outputs":[],"source":["n_observations = env.observation_space\n","print(\"Environment has observation space =\", n_observations)"]},{"cell_type":"markdown","metadata":{"id":"IO4VNgIke1QJ"},"source":["Second, we consider the action space. At every time step, the agent can move either **right** or **left**. Again, we can confirm the size of the action space by querying the environment:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1627380445697,"user":{"displayName":"Thanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg44wFhQxb1lNqczFaNjv6DfabszK-OJIJ4ACM6uw=s64","userId":"07489558485061570174"},"user_tz":-420},"id":"T7vM5R49erg4","outputId":"8758450b-6800-4535-a15d-f5655cdb7087"},"outputs":[],"source":["n_actions = env.action_space.n\n","print(\"Number of possible actions that the agent can choose from =\", n_actions)"]},{"cell_type":"markdown","metadata":{"id":"wyVmPL96sTLP"},"source":["### Solved Requirements"]},{"cell_type":"markdown","metadata":{"id":"2Dmqj7vssVbg"},"source":["Considered solved when the average reward is **greater than or equal to 450.0 over 100 consecutive trials**."]},{"cell_type":"markdown","metadata":{"id":"YMpuHr1XhFTO"},"source":["### Packages"]},{"cell_type":"markdown","metadata":{"id":"J8wysgcOh5vT"},"source":["Let’s import the needed packages. In this lab, you will use the following from PyTorch: \n","\n","- neural networks (`torch.nn`)\n","- optimization (`torch.optim`)\n","- automatic differentiation (`torch.autograd`)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tUn_wZkehHI6"},"outputs":[],"source":["import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple, deque\n","from itertools import count\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch import randint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uN-xkAORiQzg"},"outputs":[],"source":["env = gym.make('CartPole-v1')\n","\n","# set up matplotlib\n","is_ipython = 'inline' in matplotlib.get_backend()\n","if is_ipython:\n","    from IPython import display\n","\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"ThEeUt_NgVIe"},"source":["## The Behavior of a Random Agent\n"]},{"cell_type":"markdown","metadata":{"id":"Om5h9vRagcFM"},"source":["We will first check the average reward that a random agent can earn. By **Random Agent**, we're referring to an agent selecting actions randomly (i.e. without using any environmental information). Running this snippet gave an average reward in range *(20, 25)*. It may vary slightly in your case. But still, the problem is far from solved."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1627380446069,"user":{"displayName":"Thanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg44wFhQxb1lNqczFaNjv6DfabszK-OJIJ4ACM6uw=s64","userId":"07489558485061570174"},"user_tz":-420},"id":"5GIxjV6Je3Tm","outputId":"36853b0d-e636-46c6-e0a7-9f2499a7f371"},"outputs":[],"source":["rew_arr = []\n","episode_count = 100\n","for i in range(episode_count):\n","    obs, done, rew = env.reset(), False, 0\n","    while (done != True) :\n","        A =  randint(0, env.action_space.n, (1,))\n","        obs, reward, done, info = env.step(A.item())\n","        rew += reward\n","    rew_arr.append(rew)\n","    \n","print(\"Average reward per episode :\",sum(rew_arr)/ len(rew_arr))"]},{"cell_type":"markdown","metadata":{"id":"d_WFvzryuugh"},"source":["Now, let's define our model. But first, let's quickly recap what a DQN is.\n","## DQN algorithm\n","\n","Our environment is deterministic, so all equations presented here are\n","also formulated deterministically for the sake of simplicity. In the\n","reinforcement learning literature, they would also contain expectations\n","over stochastic transitions in the environment.\n","\n","Our aim will be to train a policy that tries to maximize the discounted,\n","cumulative reward\n","$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where\n","$R_{t_0}$ is also known as the *return*. The discount,\n","$\\gamma$, should be a constant between $0$ and $1$\n","that ensures the sum converges. It makes rewards from the uncertain far\n","future less important for our agent than the ones in the near future\n","that it can be fairly confident about.\n","\n","The main idea behind Q-learning is that if we had a function\n","$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell\n","us what our return would be, if we were to take an action in a given\n","state, then we could easily construct a policy that maximizes our\n","rewards:\n","\n","$$\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}$$\n","\n","However, we don't know everything about the world, so we don't have\n","access to $Q^*$. But, since neural networks are universal function\n","approximators, we can simply create one and train it to resemble\n","$Q^*$.\n","\n","For our training update rule, we'll use a fact that every $Q$\n","function for some policy obeys the Bellman equation:\n","\n","$$\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}$$\n","\n","The difference between the two sides of the equality is known as the\n","temporal difference error, $\\delta$:\n","\n","$$\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}$$\n","\n","To minimise this error, we will use the [MSE\n","loss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html). \n"]},{"cell_type":"markdown","metadata":{"id":"5_NnfkfYNEBH"},"source":["### Model Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rHXtdOmhmzb"},"outputs":[],"source":["class QNetwork(nn.Module):\n","  def __init__(self, state_size, action_size, seed):\n","    \"\"\"\n","    Build your freestyle QNetwork model.\n","    Params:\n","      - state_size (int): Dimension of each state \n","      - action_size (int): Dimension of each action\n","      - seed (int): Random seed\n","    \"\"\"\n","    super(QNetwork, self).__init__() ## calls __init__ method of nn.Module class\n","    self.seed = torch.manual_seed(seed)\n","\n","    ### YOUR CODE HERE ###\n","    None \n","    ### END ###\n","\n","  def forward(self, state):\n","    \"\"\"\n","    Build a network that maps state -> action values.\n","    \"\"\"\n","    ### YOUR CODE HERE ###\n","    return None\n","    ### END ###"]},{"cell_type":"markdown","metadata":{"id":"voQT5lsDObcC"},"source":["### DQN Agent "]},{"cell_type":"markdown","metadata":{"id":"qRruEcuRZHSr"},"source":["In this Lab, we will use Soft update in DQN network.\n","\n","![](https://greentec.github.io/images/rl3_7-en.png)\n","\n","A **soft update** means that we do not update this target network at once, but **frequently and very little**. The value of $τ$ is used. In Deepmind’s paper, which proposed an algorithm called DPG, they used $τ=0.001$. The target network is updated as follows:\n","\n","\n","$$θ_{target} = τ*θ_{local} + (1 - τ)*θ_{target}$$\n","\n","The target network will move slightly to the value of Q-network. Since the value of $τ$ is small, the update should be frequent so that the effect will be noticeable. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"2prCh8FZOlRo"},"source":["#### Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMsLMwj0OZ8f"},"outputs":[],"source":["BUFFER_SIZE = int(1e5)   # replay buffer size\n","BATCH_SIZE = 64          # minibatch size\n","GAMMA = 0.99             # discount factor\n","TAU = 1e-3               # for soft update of target parameters\n","UPDATE_EVERY = 4         # how often to update the network"]},{"cell_type":"markdown","metadata":{"id":"fxqW8ooI7FvL"},"source":["#### Agent "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0V8QH2HcKOWW"},"outputs":[],"source":["class Agent():\n","  \"\"\"\n","  Interacts with and learns form environment.\n","  \"\"\"\n","  \n","  def __init__(self, state_size, action_size, seed):\n","    \"\"\"\n","    Initialize an Agent object.\n","    Params:\n","      - state_size (int): dimension of each state\n","      - action_size (int): dimension of each action\n","      - seed (int): random seed\n","    \"\"\"\n","\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.seed = random.seed(seed)\n","\n","    # Q-Network\n","    self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n","    self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n","\n","    self.optimizer = optim.Adam(self.qnetwork_local.parameters())\n","\n","    # Replay Memory\n","    self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n","\n","    # Initialize time step (for updating every UPDATE_EVERY steps)\n","    self.t_step = 0\n","\n","  def step(self, state, action, reward, next_step, done):\n","    # Save experience in replay memory\n","    self.memory.add(state, action, reward, next_step, done)\n","\n","    # Learn every UPDATE_EVERY time steps.\n","    self.t_step = (self.t_step+1) % UPDATE_EVERY\n","    if self.t_step == 0:\n","      # If enough samples are available in memory, get random subset and learn\n","      if len(self.memory) > BATCH_SIZE:\n","        experience = self.memory.sample()\n","        self.learn(experience, GAMMA)\n","\n","\n","  def act(self, state, eps = 0):\n","    \"\"\"\n","    Returns action for given state as per current policy.\n","    Params:\n","      - state (array_like): current state\n","      - eps (float): epsilon, for epsilon-greedy action selection\n","    \"\"\" \n","    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n","    self.qnetwork_local.eval()\n","    with torch.no_grad():\n","      action_values = self.qnetwork_local(state)\n","    self.qnetwork_local.train()\n","\n","    # Epsilon-greedy action selction\n","    if random.random() > eps:\n","      return np.argmax(action_values.cpu().data.numpy())\n","    else:\n","      return random.choice(np.arange(self.action_size))\n","\n","  def learn(self, experiences, gamma):\n","    \"\"\"\n","    Update value parameters using given batch of experience tuples.\n","    Params:\n","      - experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n","      - gamma (float): discount factor\n","    \"\"\"\n","    states, actions, rewards, next_states, dones = experiences\n","    \n","    ### YOUR CODE HERE ###\n","    ## TODO: compute and minimize the loss\n","    # Get max predicted Q values (for next states) from target model\n","    None\n","\n","    # Compute Q targets for current states \n","    None\n","\n","    # Get expected Q values from local model\n","    None\n","\n","    # Compute loss\n","    None\n","\n","    # Minimize the loss\n","    self.optimizer.zero_grad()\n","    loss.backward()\n","    self.optimizer.step()\n","    ### END ###\n","    \n","    # ------------------- update target network ------------------- #\n","    self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n","\n","  def soft_update(self, local_model, target_model, tau): \n","    \"\"\"\n","    Soft update model parameters.\n","    θ_target = τ*θ_local + (1 - τ)*θ_target\n","    \n","    Params:\n","      - local model (PyTorch model): weights will be copied from\n","      - target model (PyTorch model): weights will be copied to\n","      - tau (float): interpolation parameter\n","    \"\"\"\n","    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n","      target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)"]},{"cell_type":"markdown","metadata":{"id":"zikmEzvH332m"},"source":["#### Replay Memory\n"]},{"cell_type":"markdown","metadata":{"id":"YAcHIejd3-32"},"source":["We’ll be using experience replay memory for training our DQN. It stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8qjoRljuRsF0"},"outputs":[],"source":["class ReplayBuffer:\n","  \"\"\"\n","  Fixed-size buffe to store experience tuples.\n","  \"\"\"\n","  def __init__(self, action_size, buffer_size, batch_size, seed):\n","    \"\"\"\n","    Initialize a ReplayBuffer object.\n","    Params:\n","      - action_size (int): dimension of each action\n","      - buffer_size (int): maximum size of buffer\n","      - batch_size (int): size of each training batch\n","      - seed (int): random seed\n","    \"\"\"\n","    self.action_size = action_size\n","    self.memory = deque(maxlen=buffer_size)\n","    self.batch_size = batch_size\n","    self.experiences = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n","\n","    self.seed = random.seed(seed)\n","\n","  def add(self,state, action, reward, next_state,done):\n","    \"\"\"\n","    Add a new experience to memory.\n","    \"\"\"\n","    e = self.experiences(state,action,reward,next_state,done)\n","    self.memory.append(e)\n","\n","  def sample(self):\n","    \"\"\"\n","    Randomly sample a batch of experiences from memory.\n","    \"\"\"\n","    experiences = random.sample(self.memory, k=self.batch_size)\n","\n","    states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n","    actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n","    rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n","    next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n","    dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n","\n","    return (states,actions,rewards,next_states,dones)\n","  \n","  def __len__(self):\n","    \"\"\"\n","    Return the current size of internal memory.\n","    \"\"\"\n","    return len(self.memory)"]},{"cell_type":"markdown","metadata":{"id":"uftguFHDTIJ4"},"source":["### Train the Agent with DQN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJ4T0h7TW03d"},"outputs":[],"source":["# Init agent \n","agent = Agent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8jvWm3FuSp4_"},"outputs":[],"source":["def DQN(n_episodes= 10000, eps_start=0.9, eps_end = 0.01, eps_decay=0.9):\n","  \"\"\"\n","  Deep Q-Learning\n","  Params:\n","    - n_episodes (int): maximum number of training epsiodes\n","    - eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n","    - eps_end (float): minimum value of epsilon \n","    - eps_decay (float): mutiplicative factor (per episode) for decreasing epsilon\n","  \"\"\"\n","  scores = [] # list containing score from each episode\n","  scores_window = deque(maxlen=100)  # last 100 scores\n","  eps = eps_start\n","  for i_episode in range(n_episodes):\n","    state, score, done = env.reset(), 0, False\n","    \n","    while(done != True):\n","      action = agent.act(state, eps)\n","      next_state, reward, done, _ = env.step(action)\n","      agent.step(state, action, reward, next_state, done)\n","      ## above step decides whether we will train(learn) the network\n","      ## actor (local_qnetwork) or we will fill the replay buffer\n","      ## if len replay buffer is equal to the batch size then we will\n","      ## train the network or otherwise we will add experience tuple in our replay buffer.\n","      \n","      state = next_state\n","      score += reward\n","      # decrease the epsilon\n","      eps = max(eps*eps_decay, eps_end) \n","\n","      # print('\\rEpisode {}\\tScore {:.2f}'.format(i_episode, np.mean(score)), end=\"\")\n","\n","    scores.append(score)\n","    scores_window.append(score)\n","    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n","    if np.mean(scores_window) >= 450.0:\n","      print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n","      torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n","      break\n","  return scores"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":423564,"status":"ok","timestamp":1627380873147,"user":{"displayName":"Thanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg44wFhQxb1lNqczFaNjv6DfabszK-OJIJ4ACM6uw=s64","userId":"07489558485061570174"},"user_tz":-420},"id":"mWheIbVMTYYO","outputId":"7eb7b2e9-3a8e-409c-c775-f9488ce28c04"},"outputs":[],"source":["scores = DQN()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"elapsed":667,"status":"ok","timestamp":1627380873798,"user":{"displayName":"Thanh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg44wFhQxb1lNqczFaNjv6DfabszK-OJIJ4ACM6uw=s64","userId":"07489558485061570174"},"user_tz":-420},"id":"JhFXUIj6YQhk","outputId":"b2d4e444-5141-458b-ccb8-97e4df05aaae"},"outputs":[],"source":["#plot the scores\n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","plt.plot(np.arange(len(scores)),scores)\n","plt.ylabel('Score')\n","plt.xlabel('Epsiode #')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"1iKpGFG96-Ix"},"source":["## Explore \n","\n","In this exercise, you have implemented a DQN agent and demonstrated how to use it to solve an OpenAI Gym environment. To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n","\n","- Amend the various hyperparameters and network architecture to see if you can get your agent to solve the environment faster. Once you build intuition for the hyperparameters that work well with this environment, try solving a different OpenAI Gym task with discrete actions!\n","- You may like to implement some improvements such as prioritized experience replay, Double DQN, or Dueling DQN!\n","- Write a blog post explaining the intuition behind the DQN algorithm and demonstrating how to use it to solve an RL environment of your choosing."]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOzg7nA5i+XcF28qnszFddq","collapsed_sections":[],"name":"[Template] DQN.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
