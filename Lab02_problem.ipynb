{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab02_problem.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP9bNU1nw0Axyvt7TvbgBdR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LtzUaG3Miu_i"},"source":["# Lab 02\n","\n","> In this lab, you're going to:\n","\n","1.   Implement Q agent and SARSA agent\n","2.   Study about on-policy, off-policy and how they trade-off convergence speed for sample-efficiency\n","3.   Create simple plot to visualize your agent's results\n","\n","> **NOTICE 1**: This lab calls to the ***MABe_agent*** class of previous lab. **IF** you have not finish the previous lab, you should not be aable to complete this lab\n","\n","> **NOTICE 2**: Convert your previous lab's solution or instructor's solution to format **.py** then place that file in this file's directory. We're going to inherit some classes.\n","\n","> **NOTICE 3**: Due to the inheritance of the given lab 01's solution, you can easily implement Q-learning and SARSA by changing the estimation fomula.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bVH1mGgaka7G"},"source":["Also this lab, we're going to reuse last week's environment: **GridWorld**. The different is our agent. If you don't remember what is GridWorld, please revise your last lab excercise in advance"]},{"cell_type":"code","metadata":{"id":"nQwgDnSjldGe"},"source":["from lab01 import environment, MABe_agent"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g0wkrsYVLg8k"},"source":["Estimation updates:\n",">Q-learning:\n","$Q(S_t,A_t) = (1-\\alpha)Q(S_t,A_t) + \\alpha(R + \\gamma max_a Q(S_{t+1},a))$\n","\n",">SARSA:\n","$Q(S_t, A_t) = (1-\\alpha)Q(S_t,A_t) + \\alpha(R + \\gamma Q(S_{t+1},A_{t+1}))$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"n0PoVStoSDcf"},"source":["---\n","On-policy and Off-policy \n","---\n","1. On-policy:\n","  \n","  **On-policy** is a group of reinforcement learning methods that **use the value resulting from a different policy to update the current policy**\n","\n","2. Off-policy:\n","\n","  **Off-policy** is a group of reinforcement learning methods that **use the value resulting directly from the current policy to update itself**\n","\n","3. On-policy, off-policy, cons & pros:\n","\n","*   Off-policy methods are usually learning more about the environment they can use multiple policies' experience. Thus, they're more data-sampling efficient. Famous: *Q-learning, DQN and its extensions*\n","\n","*   On-policy methods are usually more realistic (most of the time, we can only depend on our own policy for the data we can collect, especially in policy-based methods) and fast converging but not as sampling-efficient as off-policy methods\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xP_2jnmGOf1S"},"source":["## Q-learning method\n","\n","In this section, we're going to implement tabular Q-learning agent and see its performance.\n","\n","Q-agent inherits from MABe-agent we implemented earlier in this course"]},{"cell_type":"code","metadata":{"id":"uDI1TrdRLaoF"},"source":["# Implement your agent\n","class Q_agent(MABe_agent):\n","  def __init__(envir, init_location, epsilon):\n","    super(Q_agent, self).__init__(envir, init_location, epsilon)\n","    # TODO: initialize your Q table\n","    self.Q_table = None\n","\n","  # Overide method\n","  def getAction(self, observation):\n","    # TODO: return your action\n","    self.location_now, action_space, pre_reward = observation\n","    # NOTICE: the first observation is (NONE, [0,1,2,3], None)\n","    # You should process the 'None' value\n","    if self.location_now is not None:\n","      self.location_now = location_now\n","\n","    if pre_reward is not None:\n","      self.reward_trace.append(pre_reward)\n","      \n","    # example: get random action\n","    action = np.random.choice(action_space, p=[1/(len(action_space)) for action in action_space])\n","\n","    # Assert valid action\n","    assert action in action_space, \"INVALID action taken\"\n","    return action"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LRf_EN4bQZja"},"source":["# Create environment\n","Envir = environment(8,8)\n","Envir.map_Designate(17,56,-15)\n","Envir.map_Designate(18,56,-15)\n","Envir.map_Designate(19,56,-15)\n","Envir.map_Designate(21,56,-15)\n","Envir.map_Designate(25,56,-15)\n","Envir.map_Designate(33,56,-15)\n","Envir.map_Designate(41,56,-15)\n","Envir.map_Designate(42,56,-15)\n","Envir.map_Designate(43,56,-15)\n","Envir.map_Designate(46,56,-15)\n","Envir.map_Designate(47,56,-15)\n","Envir.map_Designate(47,56,-15)\n","Envir.map_Designate(15,56,+15)\n","Envir.map_Designate(1,10,+5)\n","Envir.map_Designate(26,56,+20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HWBH2YhtQoed"},"source":["init_location=0\n","dummy_q_agent = Q_agent(envir=Envir, init_location=init_location)\n","\n","num_iter = 1000\n","\n","log_freq = 100\n","Data_plot1 = []\n","\n","for i in range(num_iter):\n","  env_observation = (init_location, Envir.action_space, None)\n","  if i > 0:\n","    env_observation = Envir.get_Observation(location=dummyAgent.location_now, action=chosen_action)\n","\n","  chosen_action = dummyAgent.getAction(observation=env_observation)\n","  if (i + 1) % log_freq == 0:\n","    aver = np.mean(dummyAgent.reward_trace)\n","    Data_plot1.append(aver)\n","    print('iter: ' + str(i + 1) + '\\t Total reward: ' + str(dummyAgent.get_TotalReward()) + '\\t Average: ' + str(aver))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6C3JUidjO6bX"},"source":["## expected - SARSA\n","\n","SARSA is the on-policy version of Q-learning\n","The different is observable through its updating rule\n","\n","> $Q(S_t, A_t) = (1-\\alpha)Q(S_t,A_t) + \\alpha(R + \\gamma Q(S_{t+1},A_{t+1}))$\n","\n","Meaning: the sequence of observation is\n","> $S_t \\to A_t \\to R \\to S_{t+1} \\to A_{t+1}$ \n","\n","*HENCE*: **S-A-R-S-A**\n","\n","However, if SARSA is greedy to its value, SARSA returns to Q-learning. Thus, in this exercise, we're implementing **expected SARSA** \n"]},{"cell_type":"code","metadata":{"id":"xvvkuqyxObLg"},"source":["# Implement your agent\n","class SARSA_agent(MABe_agent):\n","  def __init__(envir, init_location, epsilon):\n","    super(Q_agent, self).__init__(envir, init_location, epsilon)\n","    # TODO: initialize your Q table\n","    self.Q_table = None\n","\n","  # Overide method\n","  def getAction(self, observation):\n","    # TODO: return your action\n","    self.location_now, action_space, pre_reward = observation\n","    # NOTICE: the first observation is (NONE, [0,1,2,3], None)\n","    # You should process the 'None' value\n","    if self.location_now is not None:\n","      self.location_now = location_now\n","\n","    if pre_reward is not None:\n","      self.reward_trace.append(pre_reward)\n","      \n","    # example: get random action\n","    action = np.random.choice(action_space, p=[1/(len(action_space)) for action in action_space])\n","\n","    # Assert valid action\n","    assert action in action_space, \"INVALID action taken\"\n","    return action"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hdUcqxuqQ5sk"},"source":["init_location=0\n","dummy_sarsa_agent = SARSA_agent(envir=Envir, init_location=init_location)\n","\n","num_iter = 1000\n","\n","log_freq = 100\n","Data_plot2 = []\n","\n","for i in range(num_iter):\n","  env_observation = (init_location, Envir.action_space, None)\n","  if i > 0:\n","    env_observation = Envir.get_Observation(location=dummyAgent.location_now, action=chosen_action)\n","\n","  chosen_action = dummyAgent.getAction(observation=env_observation)\n","  if (i + 1) % log_freq == 0:\n","    aver = np.mean(dummyAgent.reward_trace)\n","    Data_plot2.append(aver)\n","    print('iter: ' + str(i + 1) + '\\t Total reward: ' + str(dummyAgent.get_TotalReward()) + '\\t Average: ' + str(aver))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jjmygHQSRDHb"},"source":["## Plot your results\n","\n","Compare Q-learning's and SARSA's performance"]},{"cell_type":"code","metadata":{"id":"Q7_fNYmLRNvU"},"source":["import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5jKdHm1aR5XK"},"source":["# TODO: visualize your agent's performance"],"execution_count":null,"outputs":[]}]}